{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de040f9c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27ee3dc",
   "metadata": {},
   "source": [
    "# <center> Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbf620",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Logistic regression is a supervised learning classification algorithm. The model estimates the probability of an instance belonging to a class. Logistic regression is also a typical Generalized Linear Model, resulting from flexibility of the assumptions of linear regression models.\n",
    "\n",
    "While in a linear regression model we look for the parameters that best fit a straight line to the data, in logistic regression we look for the parameters that best fit a sigmoid curve to the data.\n",
    "\n",
    "* #### Motivation\n",
    "\n",
    "Could we use linear regression for a classification problem where the target variable has two classes? We would get some result, but with some problems.\n",
    "\n",
    "Linear regression models do not handle classification problems well for two main reasons:\n",
    "\n",
    "* The estimated line does not fit the data well\n",
    "* Can have outputs with values less than 0 and greater than 1\n",
    "\n",
    "The sigmoid function meets these two characteristics well, that is, it is bounded between 0 and 1 and the 'S' shape fits better to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf00f2",
   "metadata": {},
   "source": [
    "## Some math on Logistic Regression\n",
    "\n",
    "\n",
    "The estimation of the model is done by estimating the parameters of the function of a sigmoid curve that best fits the data, that is, estimating the parameters that minimize forecasting errors.\n",
    "\n",
    "Like linear regression, the logistic regression model calculates the weighted sum of input characteristics plus bias. However, instead of generating the output directly as a linear regression model, it generates the logistic output of that model.\n",
    "\n",
    "First let's see what we want to get and then build the path.\n",
    "\n",
    "## $ \\frac{\\partial logL(\\beta)}{\\partial \\beta_{j}} = \\frac{1}{m }\\sum\\limits _{i=1} ^{m}(\\sigma(\\beta^Tx_{i}) - y_{i})x_{ij} = 0 $  \n",
    "\n",
    "or\n",
    "\n",
    "## $ \\frac{\\partial logL(\\beta)}{\\partial \\beta_{j}} = \\frac{1}{m }\\sum\\limits _{i=1} ^{m}(\\frac{e^{\\beta^{T}x_{i}}}{1 + e^{\\beta^{T}x_{i}}} - y_{i})x_{ij} = 0 $  \n",
    "\n",
    "Taking the partial derivatives of each parameter for each $ x_i $ is to minimize the cost function.\n",
    "But, there is no known closed equation for calculating the value of $ \\beta $ that minimizes this cost function. However, this function is convex, so applying gradient descent can certainly find the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e9760b",
   "metadata": {},
   "source": [
    "### How to get here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7092804",
   "metadata": {},
   "source": [
    "1 - Sigmoide function\n",
    "\n",
    "## $ \\sigma = \\frac{e^x}{1 + e^x} $\n",
    "\n",
    "---\n",
    "2 - Logistic function\n",
    "\n",
    "### $ p(y=1|x) = \\frac{e^{\\beta^{T}x_{i}}}{1 + e^{\\beta^{T}x_{i}}} = \\frac{e^{(\\hat{\\beta_{0}} + \\hat{\\beta_{1}}x_{1} + ... + \\hat{\\beta_{n}}x_{n})}}{1 + e^{(\\hat{\\beta_{0}} + \\hat{\\beta_{1}}x_{1} + ... + \\hat{\\beta_{n}}x_{n})}}$ \n",
    "\n",
    "### $ p(y0|x) = \\frac{1}{1 + e^{\\beta^{T}x_{i}}} = \\frac{1}{1 + e^{(\\hat{\\beta_{0}} + \\hat{\\beta_{1}}x_{1} + ... + \\hat{\\beta_{n}}x_{n})}} $\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb30651",
   "metadata": {},
   "source": [
    "3 - Remember Bernoulli\n",
    "\n",
    "### $ p(x=y_{i}) = p^{y_{i}}(1-p)^{1-y_{i}}  $\n",
    "if $ y_i = 1 $ then p\n",
    "\n",
    "if $ y_i = 0 $ then 1 - p\n",
    "\n",
    "---\n",
    "4 - Write Bernoulli for each observation\n",
    "\n",
    "### $ L(\\beta) = \\prod\\limits _{i=1} ^{n}p^{y_i}(1-p)^{1 - y_i} $\n",
    "\n",
    "Remember we are looking for the cost function and we will need to differentiate it. So it's easier to replace the multiplications by sum by taking the logarithm.\n",
    "\n",
    "---\n",
    "5 - Taking log\n",
    "### $ logL(\\beta) = \\sum\\limits _{i=1}  ^{n} y_{i}log(p) + (1-y_{i})log(1-p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2fbedc",
   "metadata": {},
   "source": [
    "---\n",
    "6 - Some algebra\n",
    "### $ logL(\\beta) = \\sum\\limits _{i=1}  ^{n} y_{i}log(\\frac{e^{\\beta^{T}x_{i}}}{1 + e^{\\beta^{T}x_{i}}}) + (1-y_{i})log(\\frac{1}{1 + e^{\\beta^{T}x_{i}}})$\n",
    "\n",
    "### $ logL(\\beta) = \\sum\\limits _{i=1}  ^{n} y_{i}log\\frac{e^{\\beta^{T}x_{i}}}{1 + e^{\\beta^{T}x_{i}}} + \\sum\\limits _{i=1}  ^{n} (1-y_{i})log\\frac{1}{1 + e^{\\beta^{T}x_{i}}}$\n",
    "\n",
    "### $ logL(\\beta) = \\sum\\limits _{i=1 y=1}  ^{n} log\\frac{e^{\\beta^{T}x_{i}}}{1 + e^{\\beta^{T}x_{i}}} + \\sum\\limits _{i=1 y=0}  ^{n} log\\frac{1}{1 + e^{\\beta^{T}x_{i}}}$\n",
    "\n",
    "### $ logL(\\beta) = \\sum\\limits _{i=1 y=1}  ^{n} log{e^{\\beta^{T}x_{i}}} - log{(1 + e^{\\beta^{T}x_{i}})} + \\sum\\limits _{i=1 y=0}  ^{n} log{1} - log{(1 + e^{\\beta^{T}x_{i}})}$\n",
    "\n",
    "### $ logL(\\beta) = \\sum\\limits _{i=1 y=1}  ^{n} \\beta^{T}x_{i} - log{(1 + e^{\\beta^{T}x_{i}})} + \\sum\\limits _{i=1 y=0}  ^{n} 0 - log{(1 + e^{\\beta^{T}x_{i}})}$\n",
    "\n",
    "### $ logL(\\beta) = \\sum\\limits _{i=1 y=1}  ^{n} \\beta^{T}x_{i} - log{(1 + e^{\\beta^{T}x_{i}})} - \\sum\\limits _{i=1 y=0}  ^{n} log{(1 + e^{\\beta^{T}x_{i}})}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd862e",
   "metadata": {},
   "source": [
    "---\n",
    "7 - Taking partial derivatives\n",
    "\n",
    "## $ \\frac{\\partial logL(\\beta)}{\\partial \\beta} = \\sum\\limits _{i=1 y=1}  ^{n} x_{i} - \\frac{1}{1 + e^{\\beta^{T}x_{i}}}x_{i} e^{\\beta^{T}x_{i}}) - \\sum\\limits _{i=1 y=0}  ^{n} \\frac{1}{1 + e^{\\beta^{T}x_{i}}}x_{i} e^{\\beta^{T}x_{i}})$\n",
    "\n",
    "## $ \\frac{\\partial logL(\\beta)}{\\partial \\beta} = \\sum\\limits _{i=1} ^{m}(y_{i} - \\frac{e^{\\beta^{T}x_{i}}}{1 + e^{\\beta^{T}x_{i}}})x_{i} = 0 $  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a6d07",
   "metadata": {},
   "source": [
    "The output of this function represents the log odds ratio. Therefore, a variation in $ x_i $ of 1 unit modifies the log odds ratio according to the parameter. If, for example, the parameter is 2, a variation of 1 unit in xi would double the log odds ratio.\n",
    "\n",
    "Taking the antilogarithm in the sigmoid function we will directly have the probability that an instance belongs to a class. If we define a limit to reduce the outputs to 0 and 1, we have a binary classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4287d7f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83ea41",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f711e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7ec06",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c80a70",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed28f144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ebaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "# Define X and y\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "# Separate train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9188b572",
   "metadata": {},
   "source": [
    "## From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1559b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from my_LinearModels import my_LogisticRegression, accuracy, confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e55e4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "clf = my_LogisticRegression()\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515a7cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42</td>\n",
       "      <td>72</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1  All\n",
       "Actual                \n",
       "0          39   6   45\n",
       "1           3  66   69\n",
       "All        42  72  114"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusionMatrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d2735",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a1a1f",
   "metadata": {},
   "source": [
    "## From sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95a521b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "addf36d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9385964912280702"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "predict = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc29773a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.87      0.92        45\n",
      "           1       0.92      0.99      0.95        69\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.95      0.93      0.93       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report \n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9c88f",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
